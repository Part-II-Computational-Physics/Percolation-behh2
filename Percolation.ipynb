{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ccd9db7",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "Percolation is a simple model in statistical physics that has been well studied. It has a wide range of uses including modelling resistor networks, ecological disturbances (like forest fires), epidemics, and was first used to model fluid flow through porous materials.\n",
    "<br><br> Percolation theory describes the behaviour of _clusters_ in a large lattice of dimension $d$. The two most studied percolation models are site percolation and bond percolation. In site percolation, the lattice vertices are the relevant quantitities. Each lattice vertex/site is either \"occupied\", with probability $p$, or \"unoccupied\", with probability 1-$p$. A cluster is a group of occupied lattice sites that are connected by a chain of nearest neigbour links (i.e for every site in a cluster, one of its nearest neigbours is also occupied) [1]. \n",
    "<br><br> Bond percolation is a similar model where the lattice bonds are the relevant quantities. Each bond is either \"occupied\", with probability $p$, or \"unoccupied\", with probability 1-$p$. In both models, the probability of a site/bond being occupied is independent of the status of its neighbours. The system is said to _percolate_ when the largest cluster spans the lattice (i.e. it extends from one lattice boundary to the opposite boundary).\n",
    "<br><br>The critical proabibility value, $p_{c}$, is the probability at which the system can first percolate. For bond percolation in two dimensions, $p_{c} = \\frac{1}{2}$. No analytic expression exists for site percolation in two dimensions, or for either model in 3 dimensions or above [2]. \n",
    "<br><br>Percolation is a random process. Different percolation lattices will contain clusters of different shapes and sizes - we wish to discuss their average properties. We often want to find the value of some observable $Q(p)$ (e.g. average cluster size) over a range of values of $p$. In this report we will explore the most popular algorithm for doing this, the Newman-Ziff algorithm.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f7dfdd4",
   "metadata": {},
   "source": [
    "# 2. Background theory\n",
    "\n",
    "For percolation on an infinite lattice, the critical probability $p_{c}$ marks the position of a phase transition. It is the probability at which an infinite cluster first appears. A typical order paramater of the phase transition is the _percolation strength_, $P_{\\infty}(p)$. This is the fraction of sites belonging to the infinite cluster.  \n",
    "<br>An important quantity in percolation theory is the cluster number, $n_{s}(p)$. This is defined as the number of clusters of size $s$ present in the lattice, divided by the total number of lattice sites. From the cluster number, we can calculate the average cluster size, $S(p)$:\n",
    "$$S(p) = \\sum\\limits_{s=1}^{\\infty}\\frac{s^{2}n_{s}(p)}{p}$$\n",
    "<br>Near the critical point, $S$ and $P_{\\infty}$ exhibit the following behaviour:\n",
    "$$S(p)\\sim|p-p_{c}|^{-\\gamma}$$\n",
    "$$P_{\\infty}(p)\\sim|p-p_{c}|^{\\beta}$$\n",
    "where $\\gamma$ and $\\beta$ are _critical exponents_.\n",
    "<br>Since, we cannot simulate an infinite lattice we use the theory of finite size scaling to understand the behavior of quantities near the critical probability on finite lattices [1]. \n",
    "<br><br>If a quantity $X$ is expected to scale as $|p-p_{c}|^{-x}$ for an infinite lattice, then for a lattice of linear dimension $L$ we expect:\n",
    "$$X(L,\\xi) = \\xi ^{x/\\upsilon}X_{1}(L/\\xi)$$\n",
    "where $\\xi$ is the _correlation length_.\n",
    "<br><br>Near $p_c$ the correlation length diverges and so we can work in the limit $L<<\\xi$. Therefore, near $p_c$ we expect:\n",
    "$$X\\propto L^{x/\\upsilon}$$\n",
    "<br>For a finite lattice, the probability that a system percolates $\\Pi(p)$ is no longer a step function but varies smoothly with $p$. We define the average occupation probability at which the system percolates as:\n",
    "$$p_{av} = \\int p \\frac{d\\Pi(p)}{dp} dp$$\n",
    "<br>Thus, using the above expressions, we obtain the following relations for a finite lattice of linear dimension $L$ near the critical probability $p_{c}$:\n",
    "$$|p_{av}-p_{c}|\\propto L^{-1/\\upsilon}$$\n",
    "$$P_{\\infty}(L)\\propto L^{-\\beta/\\upsilon}$$\n",
    "$$S(L)\\propto L^{\\gamma/\\upsilon}$$\n",
    "It can be shown that, in two dimensions, the critical exponents satisfy the follwing identity:\n",
    "$$\\upsilon = \\beta + \\frac{\\gamma}{2}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "62ddcaa9",
   "metadata": {},
   "source": [
    "# 3. Percolation Algorithms\n",
    "\n",
    "### 3.1 Conventional Algoritm\n",
    "\n",
    "Let us first consider the conventional direct percolation algorithm. Consider some observable $Q(p)$ that we wish to calculate over a range of values of $p$. We must perform simulations at many closely spaced values of $p$, and interpolate between measurements to obtain a continous curve of $Q(p)$. This, however, introduces error into the results and it is better instead to measure $Q$ for a fixed number $n$ of occupied sites in the range of interest. The probability of there being exactly $n$ occupied sites out of a possible $N$ sites is given by the Binomial distribution:\n",
    "<br>$$B(N,n,p) = \\binom{N}{n}p^{n}(1-p)^{N-n}$$ \n",
    "If we measure the observable for all values of $n$ (giving a set of measurements {$Q_{n}$}) we can find $Q(p)$ for all $p$:\n",
    "<br>$$Q(p)=\\sum\\limits_{n=0}^{N}B(N,n,p)Q_{n}$$\n",
    "The same expression applies for bond percolation but replacing $N$ with $M$, where $M$ is the total number of bonds. \n",
    "<br><br>For site percolation, starting with an empty lattice of $N$ sites and $M$ bonds , it takes time $O(N)$ to fill the lattice and time $O(M)$ to find all the clusters for each value of $n$. Hence, the calculation takes time $O(M+N)$ for each value of $n$ and so $O(N^{2}+MN)$ overall. For a regular lattice $M=\\frac{1}{2}zN$, where $z$ is the co-ordination number, and so $O(N^{2}+MN)$ is equivalent to $O(N^{2})$ . The algorithm complexity for bond percolation is the same as for site percolation on a regular lattice. [2]\n",
    "\n",
    "### 3.2 Newmann-Ziff Algorithm\n",
    "The Newman-Ziff Algorithm takes time $O(N)$ which is a large improvement on the $O(N^{2})$ complexity of the conventional algorithm. \n",
    "<br><br>In the conventional percolation algorithm, a new state of the lattice with $n$ occupied sites/bonds is created for each value of $n$. However, since to measure $Q(p)$ we generate states for all values of $n$ from zero to $N$, we can save time using the fact that a correct state with $n+1$ occupied sites/bonds can be derived from a correct state with $n$ occupied sites/bonds by adding one extra randomly chosen site/bond. The whole set of percolation states is derived from an empty lattice by adding sites/bonds one by one. This is the idea at the core of the algorithm. [3]\n",
    "<br><br>Consider the case of bond percolation. We initially have an empty lattice with all $M$ bonds unoccupied so that each site is a cluster of size 1. To begin, we must randomly choose the order which the bonds are occupied. A simple way to do this is as follows:  \n",
    "1. Create a list of the bonds with positions labelled from 1 to $M$\n",
    "2. For $i=1$, choose a number $j$ randomly in the range $i\\le j\\le M$ \n",
    "3. Exchange the bonds at positions $i$ and $j$\n",
    "4. Repeat this for all $i$ in the range $1\\le i\\le M$\n",
    "\n",
    "This generates all possible permutations of bond orders with equal probability and has complexity $O(M)$ [2].\n",
    "<br><br>We can then occupy the bonds in the order that has been chosen. To keep track of the cluster configuration of the lattice we use a _union/find_ algorithm. When a new bond is occupied, these algorithms find the clusters the sites at either end of the bond are part of. If the sites are part of different clusters, the two clusters are combined into a single cluster.\n",
    "In their paper of 2001 [2] Newman and Ziff discuss different union/find algorithms. The most efficient one is a weighted union/find algorithm with path compression.\n",
    "<br><br>This makes use of _tree data structures_. These are hierarchical structures that are used to represent data in a way that is easy to navigate. They consist of nodes connected by edges in a hierarchical relationship with the _root_ being the topmost node of the tree. In our case, each cluster is stored as a separate tree with each site in the cluster corresponding to a node. Each cluster has one root site (which is the root of the tree) and all other sites have a _pointer_ that points towards the root or towards another site in the cluster so that by following the pointers we can go from any site in the cluster to the root of that cluster. \n",
    "<br><br>When a new bond is added, the pointers belonging to the sites at each end of the bond are followed. If the pointers lead to the same root site, then the sites are part of the same cluster. After the path is traversed, each pointer along the path is changed to point directly to the root of the cluster. This is called _path compression_ and it speeds up the traversal of the cluster. If instead the pointers lead to different roots (so the sites are part of different clusters), the two clusters are combined by adding a pointer from the root of one cluster to the root of the other. The pointer is always orientated to point towards the larger of the two clusters. To do this, the size of a cluster (i.e number of sites) is stored at the root site of each cluster. When two clusters are combined, the size of the larger cluster is updated by adding to it the size of the smaller cluster. This process is repeated until all bonds in the lattice are occupied [2].\n",
    "<br><br>It can be shown that time taken for the union/find algorithm is $O(1)$ in lattice size. Therefore, it takes time $O(1)$ to occupy each bond and so time $O(M)$ to add all the bonds (whilst keeping track of all the clusters). Since it also takes time $O(M)$ to generate the order in which bonds are added, for bond percolation, the Newman Ziff algorithm has overall complexity $O(M)$. The algorithm is very similar for site percolation (the sites are occupied in a random order instead of the bonds) and, for a regular lattice, has complexity $O(N)$ [2].\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5ab94a7",
   "metadata": {},
   "source": [
    "# 4. Implementation of Newman-Ziff algorithm\n",
    "\n",
    "### 4.1 Programme\n",
    "We will now implement the Newman-Ziff algorithm. The code is based of that given in appendix A of the paper by Newman and Ziff (written in C) but has been modified for python [2]. The programme is for site percolation on a square lattice with $N=L\\times{L}$ sites. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9826c1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import random\n",
    "import matplotlib.animation as animation\n",
    "import scipy\n",
    "\n",
    "L = 64\n",
    "N = L**2\n",
    "Empty = -N-1\n",
    "Boundary = -N-2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e8074185",
   "metadata": {},
   "source": [
    "Sites are indexed with an integer label, taking values from 0 to $N-1$.\n",
    " <br><br>We first construct an array \"ptr\" which contains the pointer for each lattice site and stores the size of each cluster at its root site. In the array, each non-root occupied site is labelled with the index of its parent site in the tree (i.e the site its pointer points towards). Root sites are labelled with a value that is equal to minus the size of the cluster (the negative means root sites can be easily distinguished from non-root sites). All unoccupied sites takes the value of \"Empty\".  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d4d42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ptr = np.zeros(N, dtype=int)    #array of pointers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "abc0ee87",
   "metadata": {},
   "source": [
    "We define a function \"nearestneighbour\" which generates the nearest neigbours of each site and stores them in an array \"nn\". If a site is on a boundary of the lattice, its nearest neighbour (in the direction of the boundary) will have a label \"Boundary\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51edcb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = np.zeros((N,4), dtype=int) #array of nearest neighbours\n",
    "\n",
    "def nearestneighbour():\n",
    "\n",
    "    \"\"\"constructs a numpy.ndarray of the nearest neighbours of each site in a square lattice\n",
    "    with N=L*L sites\"\"\"\n",
    "\n",
    "    for i in range(N):\n",
    "        nn[i,0] = (i+1)%N\n",
    "        nn[i,1] = (i-1)%N\n",
    "        nn[i,2] = (i+L)%N\n",
    "        nn[i,3] = (i-L)%N\n",
    "\n",
    "        if i%L==0:\n",
    "            nn[i,1]=Boundary\n",
    "       \n",
    "        if (i+1)%L==0:\n",
    "            nn[i,0]=Boundary\n",
    "            \n",
    "        if np.floor(i/L)==0:\n",
    "            nn[i,3]=Boundary\n",
    "            \n",
    "        if np.floor(i/L)==L-1:\n",
    "            nn[i,2]=Boundary\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d6ddcfa",
   "metadata": {},
   "source": [
    "We also construct a function \"permutate\" which creates a random permutation of the site labels and stores them in an array \"order\". This is the order in which the sites will be occupied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e210ad2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "order = np.zeros(N, dtype=int)  #occupation order array\n",
    "\n",
    "def permutate():\n",
    "\n",
    "    \"\"\"constructs a random permutation of the site labels (integers from 0 to N-1) using the algorithm \n",
    "    given earlier and stores it in a numpy.ndarray \"\"\"\n",
    "    \n",
    "    for i in range(N):\n",
    "        order[i]=i\n",
    "\n",
    "    for i in range(N):\n",
    "        \n",
    "        #generates a random number j between i and N\n",
    "        j = random.randint(i,N-1)\n",
    "        \n",
    "        #swaps the positions of sites i and j\n",
    "        temp=order[i]\n",
    "        order[i] = order[j]\n",
    "        order[j] = temp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a0136504",
   "metadata": {},
   "source": [
    "The \"findroot\" function returns the (label of the)  root site of the cluster each site is part of and also carries out path compression. We also keep track of the number of steps taken in traversing from a site to its root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f8a5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findroot(i):\n",
    "\n",
    "    \"\"\"Recursive function that returns the label (an integer from 0 to N-1) of the root site of the cluster \n",
    "    that the input site (label i) is part of. Also carries out path compression by \n",
    "    setting the value of ptr[i] to be the (label of the) root of the tree that site i is part of . \"\"\"\n",
    "    step = 0\n",
    "\n",
    "    if ptr[i]<0:\n",
    "        return i, step\n",
    "\n",
    "    else:\n",
    "        ptr[i] = findroot(ptr[i])[0]\n",
    "        step += 1\n",
    "        return ptr[i], step\n",
    "     "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "13219d06",
   "metadata": {},
   "source": [
    "The main part of the algorithm is contained within the function \"percolate\". This occupies the lattice sites in the order given by the array \"order\" and finds the roots of the adjacent sites using the \"findroot\" function. If the sites are part of different clusters, the clusters are combined by adding the smaller cluster to the larger cluster. The function keeps track of the largest cluster in the lattice and the sum of the squared sizes of clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426d1567",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percolate():\n",
    "\n",
    "    \"\"\"Model for site percolation on a square lattice with N sites using the Newman-Ziff \n",
    "    algorithim.\"\"\"\n",
    "\n",
    "    largest = 0 #size of largest cluster\n",
    "    total = 0   #total number of clusters\n",
    "    steps = 0   #number of steps taken in traversing trees\n",
    "    n = []  #number of sites occupied\n",
    "    largest_cluster = []\n",
    "    average_cluster = []\n",
    "    sum_squared = np.zeros(N, dtype=int)\n",
    "\n",
    "    for i in range(N):\n",
    "        #constructs empty lattice\n",
    "        ptr[i] = Empty\n",
    "\n",
    "    for i in range(N):\n",
    "        #occupies sites in order\n",
    "        root_1 = site_1 = order[i] \n",
    "        ptr[site_1] = -1\n",
    "        total += 1\n",
    "        sum_squared[i] += 1\n",
    "\n",
    "\n",
    "        for j in range(4):\n",
    "            #finds nearest neighbours\n",
    "            site_2 = nn[site_1, j]\n",
    "\n",
    "            if site_2 != Boundary:\n",
    "\n",
    "                if ptr[site_2] != Empty:\n",
    "                    #If nearest neighbour is occupied, find root of cluster it is part of\n",
    "                    root_2 = findroot(site_2)[0]\n",
    "                    steps += findroot(site_2)[1]\n",
    "\n",
    "\n",
    "                    if root_2 != root_1: \n",
    "                        #If root of site and its nearest neighbour are different combine clusters\n",
    "                        #add size of smaller cluster to size of larger cluster \n",
    "                        total -= 1\n",
    "                        sum_squared[i] -= (ptr[root_2])**2\n",
    "                        sum_squared[i] -= (ptr[root_1])**2\n",
    "\n",
    "                        if ptr[root_1]>ptr[root_2]:\n",
    "                            ptr[root_2] += ptr[root_1]  \n",
    "                            ptr[root_1] = root_2\n",
    "                            root_1 = root_2\n",
    "                            sum_squared[i] += (ptr[root_2])**2\n",
    "\n",
    "                        else:\n",
    "                            ptr[root_1] += ptr[root_2]\n",
    "                            ptr[root_2] = root_1\n",
    "                            sum_squared[i] += (ptr[root_1])**2\n",
    "\n",
    "                        if -ptr[root_1] > largest:\n",
    "                            #Keeps track of largest cluster in lattice\n",
    "                            largest = -ptr[root_1]\n",
    "\n",
    "        \n",
    "        if i < N-1:\n",
    "            sum_squared[i+1] = sum_squared[i]                \n",
    "                        \n",
    "        n.append(i+1)\n",
    "        largest_cluster.append(largest)\n",
    "        average_cluster.append(sum_squared[i]/(i+1))\n",
    "\n",
    "    return n, largest_cluster, average_cluster, steps\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eccd0cec",
   "metadata": {},
   "source": [
    "### 4.2 Computing observables and critical exponents\n",
    "\n",
    "We now use our programme to compute observables of the system over a range of $p$ values. \n",
    "<br><br>We first use the algorithm to find the average occupation probability at which the system percolates, $p_{av}$ (As $N\\to \\infty$, $p_{av}\\to p_{c}$).\n",
    "<br>We start the lattice in an initial configuration where there are occupied sites along two opposite edges of the lattice (i.e there are two initial clusters). We then occupy the remaining lattice sites using the algorithm and at each step we check the roots of the two initial clusters using the \"findroot\" function. If the clusters have the same root (i.e are now part of the same cluster), then the cluster spans from one edge of the lattice to the opposite edge and so percolation has occurred. Since the union/find algorithm has complexity $O(1)$ and we only need to check two sites per step, checking for percolation in this way does not increase the complexity of the algorithm. We note that when checking for percolation using this method, we no longer have a square lattice. [2]\n",
    "<br><br>We run the programme for $k$ permutations of the occupation order and for each find the value of occupation number at which percolation occurs. Taking the mean and standard deviation of this we get an estimate of the average occupation number at which percolation occcurs, $n_{av}$, and the associated error. Dividing by $N$ gives us an estimate of $p_{av}$. The results for $k=500$ for different lattice sizes are stored in the file 'paverage.npy' and the data is plotted in the graph below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700b8489",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c21cdca5",
   "metadata": {},
   "source": [
    "From the graph we obtain $p_c = 0.593\\pm 0.02$. This is in agreement with the accepted value [1].\n",
    "<br>In section 2 we introduced the relationship $|p_{av}-p_{c}|\\propto L^{-1/\\upsilon}$. Hence, by plotting $log(|p_{av}-p_{c}|)$ against $log(L)$ we can determine the critical exponent $\\upsilon$:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5568b9fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a652478f",
   "metadata": {},
   "source": [
    "<br><br>The percolation strength, $P_{\\infty,n}$, is the fraction of sites that are part of the spanning cluster. Before percolation, $P_{\\infty,n}=0$ and after percolation, $P_{\\infty,n}$ is given by the size of the largest cluster divided by $n$ [4]. Using the function \"percolate\", we find $P_{\\infty,n}$ as a function of $n$ (averaged over 500 runs) and then perform a convolution with the Binomial distribution to give $P_{\\infty}(p)$. The data for $P_{\\infty}(p_{c})$ for different lattice sizes is stored in the file 'Percolationstrength.npy'.\n",
    "<br><br> As shown in section 2 we expect $P_{\\infty}(L)\\propto L^{-\\beta/\\upsilon}$ near $p=p_{c}$ and so plotting $log(L)$ against $log(P_{\\infty}(p_{c}))$ allows us to determine the crictical exponent $\\beta$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ee821f",
   "metadata": {},
   "outputs": [],
   "source": [
    "percolation_strength = np.load('Percolationstrength.npy')\n",
    "plt.plot(np.log(percolation_strength[0]), np.log(percolation_strength[1]), '.' )\n",
    "z = np.polyfit(np.log(percolation_strength[0]),np.log(percolation_strength[1]),1)\n",
    "f = np.poly1d(z)\n",
    "plt.plot(np.log(percolation_strength[0]), f(np.log(percolation_strength[0])), linestyle='--')\n",
    "plt.xlabel('log(L)')\n",
    "plt.ylabel('log(P_inf(p_c))')\n",
    "plt.show()\n",
    "scipy.stats.linregress(np.log(percolation_strength[0]),np.log(percolation_strength[1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "458a7541",
   "metadata": {},
   "source": [
    "The function \"percolate\" keeps track of the sum of the squared cluster sizes for each $n$. We this divide by $n$ and convolve with the binomal distribution to obtain the average cluster size as a function of $p$, $S(p)$. The values for $S(p_c)$ for different lattice sizes (averaged over 500 runs) are stored in the file 'averagecluster.npy'. We expect $S(L)\\propto L^{\\gamma/\\upsilon}$ near $p = p_{c}$ and so plotting $log(L)$ against $log(S(p_{c}))$ allows us to determine the critical exponent $\\gamma$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8667a7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_cluster = np.load('averagecluster.npy')\n",
    "plt.plot(np.log(average_cluster[0]), np.log(average_cluster[1]), '.' )\n",
    "z = np.polyfit(np.log(average_cluster[0]),np.log(average_cluster[1]),1)\n",
    "f = np.poly1d(z)\n",
    "plt.plot(np.log(average_cluster[0]), f(np.log(average_cluster[0])), linestyle='--')\n",
    "plt.xlabel('log(L)')\n",
    "plt.ylabel('log(S(p_c))')\n",
    "plt.show()\n",
    "scipy.stats.linregress(np.log(average_cluster[0]),np.log(average_cluster[1]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af204d11",
   "metadata": {},
   "source": [
    "### 4.3 Algorithm performance\n",
    "Using the \"findroot\" function, we kept track of the total number of steps, $n_{step}$, taken through trees during each run of the algorithm for different lattice sizes (averaged over 100 runs). The data is stored in the file 'totalsteps.npy'. Assuming a complexity of $O(N^{\\alpha})$, plotting $log(N)$ against $log(n_{step})$ allows us to determine the value of $\\alpha$ from the gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3338ccd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_steps = np.load('totalsteps.npy')\n",
    "plt.plot(np.log(total_steps[0]),np.log(total_steps[1]), '.')\n",
    "z = np.polyfit(np.log(total_steps[0]),np.log(total_steps[1]),1)\n",
    "f = np.poly1d(z)\n",
    "plt.plot(np.log(total_steps[0]), f(np.log(total_steps[0])), linestyle='--')\n",
    "plt.xlabel('log(N)')\n",
    "plt.ylabel('log(Number of steps)')\n",
    "plt.show()\n",
    "scipy.stats.linregress(np.log(total_steps[0]),np.log(total_steps[1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "df5b26d1",
   "metadata": {},
   "source": [
    "From the graph we obtain $\\alpha = 1.011 \\pm 0.002$. The expected complexity is $O(N)$ (i.e $\\alpha = 1$), so the result deviates very slightly from the expected behaviour. Data was only taken up to $N = 10^{5}$ (due to computation time) and so this is possibly the reason for the small deviation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e72dc7e",
   "metadata": {},
   "source": [
    "We can also find the time taken for the algorithm to run for different lattice sizes (averaged over 100 runs) using the \"timeit\" module. The data from this is stored in the file 'times.npy'. Again, assuming a complexity of $O(N^{\\alpha})$, plotting $log(N)$ against $log(time)$ gives another estimate for $\\alpha$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4cd108",
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.load('times.npy')\n",
    "plt.plot(np.log(time[0]),np.log(time[1]),'.')\n",
    "z = np.polyfit(np.log(time[0]),np.log(time[1]),1)\n",
    "f = np.poly1d(z)\n",
    "plt.plot(np.log(time[0]), f(np.log(time[0])), linestyle='--')\n",
    "plt.xlabel('log(N)')\n",
    "plt.ylabel('log(time)')\n",
    "plt.show()\n",
    "scipy.stats.linregress(np.log(time[0]),np.log(time[1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "364f636c",
   "metadata": {},
   "source": [
    "From this graph we obtain $\\alpha = 1.06 \\pm 0.02$. Here, the slight deviation from the expected complexity can be explained by the increasing incidence of cache misses as N becomes large [2]. This is due to inaccuracies in the hardware and is not caused by the algorithm itself. \n",
    "<br><br>Although the programme perfomed at complexity very close to $O(N)$, the project was still limited by computation time. On the computer used, one run of the algorithm for an $N = 1000\\times 1000$ lattice took around 40 seconds. Hence, for 1000 runs (number used in [2]) this amounts to 11 hours computation time. Comparing this to the 2.9 seconds taken for one run of an $N=1000\\times 1000$ lattice in [2], it is clear that a better quality computer would have allowed us to take readings for larger lattice sizes, and perform more runs for each lattice size, which would've improved the accuracy of the results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d76ab5ad",
   "metadata": {},
   "source": [
    "#### __References__\n",
    "\n",
    "[1] D. Stauffer and A. Aharony, _Introduction to Percolation Theory_, 2nd ed. Taylor and Francis, London, 1992\n",
    "<br>[2] Newman, MEJ, and Robert M Ziff. 2000. “Efficient Monte Carlo Algorithm and High-Precision Results for Percolation.” _Physical Review Letters_ 85 (19): 4104.\n",
    "<br>[3] Sorge, A. 2015. _The Newman–Ziff Algorithm pypercolate documentation_. Available at: https://pypercolate.readthedocs.io/en/stable/newman-ziff.html \n",
    "<br>[4] Hasenbusch J. and Wilhelm M. 2011. \"Percolation\" "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
